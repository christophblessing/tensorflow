{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Linear Regression is used to predict numeric values, like change of survival on the titanic.\n",
    "Classification means to serparate data points into classes.\n",
    "\n",
    "We have a dataset containing four different features of three different types of flowers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "train = pd.read_csv('data/iris_training.csv', names=CSV_COLUMN_NAMES, header=0)\n",
    "test = pd.read_csv('data/iris_test.csv', names=CSV_COLUMN_NAMES, header=0)\n",
    "\n",
    "# pop the species column off and use that as our label\n",
    "train_y = train.pop('Species')\n",
    "test_y = test.pop('Species')\n",
    "\n",
    "\n",
    "# Input function: Converts the inputs to a Dataset, shuffles and repeat if in training mode\n",
    "def input_fn(features, labels, training=True, batch_size=256):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(1000).repeat()\n",
    "\n",
    "    return dataset.batch(batch_size)\n",
    "\n",
    "# Feature columns describe how to use the input.\n",
    "my_feature_columns = []\n",
    "for key in train.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "Two build it models from tensor flow:\n",
    "* deep neural network classifier ```DNNClassifier```\n",
    "* linear classifier ```LinearClassifier```\n",
    "\n",
    "DNN best choise here because maybe there is no linear correspondence in the data.\n",
    "\n",
    "Changing models is easy. Most of the work is loading and preprocessing data.\n",
    "\n",
    "We build a DNN with 2 hidden layers which have 30 and 10 hidden nodes each.\n",
    "Theses numbers are given by the tutorial and seem arbitrary, a best choise is done by experimenting and tests.\n",
    "\n",
    "Since we have 3 differnent types of flowers, the model must choose between 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Using default config.\nWARNING:tensorflow:Using temporary folder as model directory: /var/folders/nz/48n490d10_dggw04m5r5z0d80000gp/T/tmpgz37qip_\nINFO:tensorflow:Using config: {'_model_dir': '/var/folders/nz/48n490d10_dggw04m5r5z0d80000gp/T/tmpgz37qip_', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa913e4a850>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
    }
   ],
   "source": [
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=[30, 10],\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "With input function as lambda, since we do not provide a make_input_function to return an inner function as before.\n",
    "Set in training mode with our training dataset and the Species as labels.\n",
    "\n",
    "Numer of steps is similar to epoch, but just means how many times the classifier has looked at things to come to an end.\n",
    "\n",
    "Output: tells the current step and the loss. The less loss the better.\n",
    "\n",
    "E.g. INFO:tensorflow:Loss for final step: 0.36024907. Is pretty high, so pretty bad.\n",
    "\n",
    "The output gets more importent to look at with bigger models containing terrabytes of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Saving checkpoints for 0 into /var/folders/nz/48n490d10_dggw04m5r5z0d80000gp/T/tmpgz37qip_/model.ckpt.\nINFO:tensorflow:loss = 1.2353456, step = 0\nINFO:tensorflow:global_step/sec: 106.97\nINFO:tensorflow:loss = 0.928645, step = 100 (0.936 sec)\nINFO:tensorflow:global_step/sec: 160.85\nINFO:tensorflow:loss = 0.87341493, step = 200 (0.622 sec)\nINFO:tensorflow:global_step/sec: 156.679\nINFO:tensorflow:loss = 0.8380406, step = 300 (0.637 sec)\nINFO:tensorflow:global_step/sec: 169.727\nINFO:tensorflow:loss = 0.7967387, step = 400 (0.589 sec)\nINFO:tensorflow:global_step/sec: 132.083\nINFO:tensorflow:loss = 0.772187, step = 500 (0.758 sec)\nINFO:tensorflow:global_step/sec: 128.662\nINFO:tensorflow:loss = 0.7380208, step = 600 (0.777 sec)\nINFO:tensorflow:global_step/sec: 143.572\nINFO:tensorflow:loss = 0.72256, step = 700 (0.697 sec)\nINFO:tensorflow:global_step/sec: 154.256\nINFO:tensorflow:loss = 0.7030907, step = 800 (0.648 sec)\nINFO:tensorflow:global_step/sec: 163.506\nINFO:tensorflow:loss = 0.6718408, step = 900 (0.612 sec)\nINFO:tensorflow:global_step/sec: 172.974\nINFO:tensorflow:loss = 0.6616173, step = 1000 (0.578 sec)\nINFO:tensorflow:global_step/sec: 160.871\nINFO:tensorflow:loss = 0.6465658, step = 1100 (0.622 sec)\nINFO:tensorflow:global_step/sec: 158.905\nINFO:tensorflow:loss = 0.63081145, step = 1200 (0.630 sec)\nINFO:tensorflow:global_step/sec: 159.018\nINFO:tensorflow:loss = 0.6114887, step = 1300 (0.628 sec)\nINFO:tensorflow:global_step/sec: 154.276\nINFO:tensorflow:loss = 0.6016828, step = 1400 (0.648 sec)\nINFO:tensorflow:global_step/sec: 165.315\nINFO:tensorflow:loss = 0.5939238, step = 1500 (0.605 sec)\nINFO:tensorflow:global_step/sec: 151.175\nINFO:tensorflow:loss = 0.5712799, step = 1600 (0.663 sec)\nINFO:tensorflow:global_step/sec: 130.718\nINFO:tensorflow:loss = 0.5742372, step = 1700 (0.765 sec)\nINFO:tensorflow:global_step/sec: 135.272\nINFO:tensorflow:loss = 0.56228024, step = 1800 (0.739 sec)\nINFO:tensorflow:global_step/sec: 157.396\nINFO:tensorflow:loss = 0.55244243, step = 1900 (0.635 sec)\nINFO:tensorflow:global_step/sec: 163.941\nINFO:tensorflow:loss = 0.5363281, step = 2000 (0.610 sec)\nINFO:tensorflow:global_step/sec: 166.146\nINFO:tensorflow:loss = 0.5370118, step = 2100 (0.602 sec)\nINFO:tensorflow:global_step/sec: 162.789\nINFO:tensorflow:loss = 0.5419373, step = 2200 (0.614 sec)\nINFO:tensorflow:global_step/sec: 167.877\nINFO:tensorflow:loss = 0.53083634, step = 2300 (0.596 sec)\nINFO:tensorflow:global_step/sec: 161.881\nINFO:tensorflow:loss = 0.5225494, step = 2400 (0.618 sec)\nINFO:tensorflow:global_step/sec: 150.783\nINFO:tensorflow:loss = 0.5091944, step = 2500 (0.663 sec)\nINFO:tensorflow:global_step/sec: 161.569\nINFO:tensorflow:loss = 0.5025276, step = 2600 (0.619 sec)\nINFO:tensorflow:global_step/sec: 151.167\nINFO:tensorflow:loss = 0.49553555, step = 2700 (0.662 sec)\nINFO:tensorflow:global_step/sec: 149.639\nINFO:tensorflow:loss = 0.4905439, step = 2800 (0.668 sec)\nINFO:tensorflow:global_step/sec: 156.982\nINFO:tensorflow:loss = 0.4867289, step = 2900 (0.637 sec)\nINFO:tensorflow:global_step/sec: 153.276\nINFO:tensorflow:loss = 0.47138193, step = 3000 (0.653 sec)\nINFO:tensorflow:global_step/sec: 160.801\nINFO:tensorflow:loss = 0.48194468, step = 3100 (0.622 sec)\nINFO:tensorflow:global_step/sec: 148.158\nINFO:tensorflow:loss = 0.47051188, step = 3200 (0.675 sec)\nINFO:tensorflow:global_step/sec: 148.286\nINFO:tensorflow:loss = 0.46363243, step = 3300 (0.674 sec)\nINFO:tensorflow:global_step/sec: 145.325\nINFO:tensorflow:loss = 0.46167535, step = 3400 (0.689 sec)\nINFO:tensorflow:global_step/sec: 155.831\nINFO:tensorflow:loss = 0.4513604, step = 3500 (0.641 sec)\nINFO:tensorflow:global_step/sec: 164.395\nINFO:tensorflow:loss = 0.44066012, step = 3600 (0.609 sec)\nINFO:tensorflow:global_step/sec: 158.552\nINFO:tensorflow:loss = 0.43394452, step = 3700 (0.630 sec)\nINFO:tensorflow:global_step/sec: 161.517\nINFO:tensorflow:loss = 0.4353891, step = 3800 (0.620 sec)\nINFO:tensorflow:global_step/sec: 153.352\nINFO:tensorflow:loss = 0.4324088, step = 3900 (0.652 sec)\nINFO:tensorflow:global_step/sec: 150.445\nINFO:tensorflow:loss = 0.44279256, step = 4000 (0.664 sec)\nINFO:tensorflow:global_step/sec: 155.081\nINFO:tensorflow:loss = 0.42075118, step = 4100 (0.644 sec)\nINFO:tensorflow:global_step/sec: 140.297\nINFO:tensorflow:loss = 0.4294589, step = 4200 (0.714 sec)\nINFO:tensorflow:global_step/sec: 136.156\nINFO:tensorflow:loss = 0.41392902, step = 4300 (0.734 sec)\nINFO:tensorflow:global_step/sec: 138.714\nINFO:tensorflow:loss = 0.41495377, step = 4400 (0.721 sec)\nINFO:tensorflow:global_step/sec: 149.573\nINFO:tensorflow:loss = 0.40798056, step = 4500 (0.669 sec)\nINFO:tensorflow:global_step/sec: 148.576\nINFO:tensorflow:loss = 0.40582305, step = 4600 (0.673 sec)\nINFO:tensorflow:global_step/sec: 140.79\nINFO:tensorflow:loss = 0.40338677, step = 4700 (0.710 sec)\nINFO:tensorflow:global_step/sec: 146.17\nINFO:tensorflow:loss = 0.39485732, step = 4800 (0.685 sec)\nINFO:tensorflow:global_step/sec: 138.906\nINFO:tensorflow:loss = 0.38978404, step = 4900 (0.720 sec)\nINFO:tensorflow:Saving checkpoints for 5000 into /var/folders/nz/48n490d10_dggw04m5r5z0d80000gp/T/tmpgz37qip_/model.ckpt.\nINFO:tensorflow:Loss for final step: 0.3808903.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifierV2 at 0x7fa913e4a350>"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda: input_fn(train, train_y),\n",
    "    steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "We use the test dataset and the test labels this time. Also set training to false.\n",
    "The result is an accuracy of 95% at 5000 steps. This seems pretty good for not knowing what I am doing.\n",
    "\n",
    "More is not always better, so I tried 100.000 steps. The Loss for final step was with 0.13823777 much lower. The accuracy changed to 93%, which is actually worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Calling model_fn.\nWARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Starting evaluation at 2020-07-14T16:36:13Z\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /var/folders/nz/48n490d10_dggw04m5r5z0d80000gp/T/tmpgz37qip_/model.ckpt-5000\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Finished evaluation at 2020-07-14-16:36:14\nINFO:tensorflow:Saving dict for global step 5000: accuracy = 0.8666667, average_loss = 0.46237597, global_step = 5000, loss = 0.46237597\nINFO:tensorflow:Saving 'checkpoint_path' summary for global step 5000: /var/folders/nz/48n490d10_dggw04m5r5z0d80000gp/T/tmpgz37qip_/model.ckpt-5000\n"
    }
   ],
   "source": [
    "eval_result = classifier.evaluate(input_fn=lambda: input_fn(test, test_y, training=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "Predictions are made by putting in a dictionary of feature value pairs.\n",
    "The function below prints out the final prediction result, which is the most likely flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "INFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from /var/folders/nz/48n490d10_dggw04m5r5z0d80000gp/T/tmpgz37qip_/model.ckpt-5000\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nPrediction is \"Setosa\" (83.6%)\nPrediction is \"Versicolor\" (47.9%)\nPrediction is \"Virginica\" (63.3%)\n"
    }
   ],
   "source": [
    "# Convert the inputs to a Dataset without labels.\n",
    "def predict_input_fn(features, batch_size=256):\n",
    "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
    "\n",
    "def print_predictions(predictions):\n",
    "    for pred_dict in predictions:\n",
    "        class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "        print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
    "            SPECIES[class_id], 100 * probability))\n",
    "\n",
    "# which are expected to be 'Setosa', 'Versicolor', 'Virginica'\n",
    "predict = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1]\n",
    "}\n",
    "\n",
    "predictions = classifier.predict(input_fn=lambda: predict_input_fn(predict))\n",
    "print_predictions(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594728671543",
   "display_name": "Python 3.7.7 64-bit ('tensorflow-sessions': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}